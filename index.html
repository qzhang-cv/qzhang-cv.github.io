<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qi Zhang</title>

  <meta name="author" content="Qi Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- <script src="https://kit.fontawesome.com/075b3b8c94.js" crossorigin="anonymous"></script> -->
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>

  <table
    style="width:100%;max-width:950px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/QiZhang.gif"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/QiZhangSmall.gif" class="hoverZoomLink"></a>
                </td>
                <td style="padding: 2.5%;width: 63%;vertical-align: middle;text-align: justify;">
                  <p style="text-align:left">
                    <name>Qi Zhang (张琦)</name>
                  </p>
                  <!-- <p>
                <subtitle>Reasercher <br> Tencent AI Lab</subtitle>
              </p> -->
                  <p>
                    I am currently a Researcher in the Visual Computing Center@<a
                      href="https://ai.tencent.com/ailab/en/index">Tencent AI Lab</a>. I do research in 3D computer
                    vision and computational photography, where my interests focus on neural radiance fields (NeRF) and
                    neural rendering, 3D reconstruction and modeling, AI generated content (AIGC), image rectifcation,
                    and light field imaging.
                  </p>
                  <!-- <p>
                I serve regularly as a reviewer for major computer vision conferences and journals including CVPR/ECCV/WACV/TOG/TPAMI.
              </p> -->
                  <p>
                    Before joining Tencent AI Lab in Jun. 2021, I received my Ph.D. degree from the School of Computer
                    Science of Northwestern Polytechnical University in 2021, I was supervised by <a
                      href="https://teacher.nwpu.edu.cn/qwang.html"> Prof. Qing Wang</a>.
                    I was a visiting student at the Australian National University (ANU) between Jul. 2019 to Aug. 2020,
                    which was supervised by <a href="http://users.cecs.anu.edu.au/~hongdong/">Prof. Hongdong Li</a>.
                    I received the <strong> <a
                        href="https://www.ccf.org.cn/Awards/Awards_Recipients/2021/yxbsxwlwjtm/">Outstanding Doctoral
                        Dissertation Award Nominee</a></strong> from China Computer Federation (CCF) in 2021. I also won
                    the 2021 ACM Xi'an Doctoral Dissertation Award and the 2023 NWPU Doctoral Dissertation Award.
                  </p>
                  <p>
                    At Tencent, I've worked on <a href="https://mp.weixin.qq.com/s/SW_PRYA1XtMRgHb8VKoV4w">4D Content
                      Generation</a> and
                    <a href="https://mp.weixin.qq.com/s/Ca-K9CrfQ3SO3JEDIu5MdQ">3D Chat on Tencetn Meeting</a> If you
                    are interested in
                    the internship about neural rendering (e.g. NeRF, SDF, inverse rendering),
                    digital avatars, and 3D generation, please feel free to contact me via e-mail and wechat.
                  </p>
                  <p style="text-align:left">
                    <a href="mailto:nwpuqzhang@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/QiZhang-CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="data/QiZhang-bio.txt">Bio</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=2vFjhHMAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                    <a href="images/wechat.jpg">WeChat</a> &nbsp/&nbsp
                    <a href="https://github.com/qzhang-cv">Github</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align: justify;">
                  <heading>News</heading>
                  <p>
                    &#127881;&#127881;2024.03: 5 papers / 9 submissions accetped to <strong style="color: brown">CVPR 2024</strong>! <br>
                    &#127881;&#127881;2024.02: <b>DINER</b> extended to <strong style="color: brown">TPAMI</strong>! <br>
                    &#127881;&#127881;2023.12: <b>NeIF </b> accetped to  <strong style="color: brown">AAAI 2023</strong><br>
                    &#127881;&#127881;2023.08: <b>LoD-NeuS </b> accetped to  <strong style="color: brown">SIGGRAPH Asia 2023</strong><br>
                    &#127881;&#127881;2023.07: <b>Pyramid NeRF</b> accetped to  <strong style="color: brown">IJCV 2023</strong><br>
                    &#127881;&#127881;2023.03: 7 papers (with 1 highlight paper) / 9 submissions accetped to <strong style="color: brown">CVPR 2023</strong>! <br>
                    &#127881;&#127881;2022.08: 1 paper (journal track) accetped to  <strong style="color: brown">SIGGRAPH Asia 2022</strong><br>
                    &#127881;&#127881;2022.03: 4 papers / 4 submissions accetped to  <strong style="color: brown">CVPR 2022</strong><br>
                    &#127881;&#127881;2021.12: CCF Outstanding Doctoral Dissertation Award Nominee
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle;text-align: justify;">
                  <heading>Research</heading>
                  <p>
                    Please find below a complete list of my publications with representative papers <span
                      class="highlight">highlighted</span>. The IEEE TPAMI, IJCV are top journals in the field of
                    computer vision and computational photography. The CVPR is the premier conference in Computer Vision
                    research community.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- GS-IR -->
              <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="paper-box">
                  <div class="badge">CVPR 2024</div>
                  <img src="images/gs-ir-teaser.jpg" width="300">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <papertitle>GS-IR: 3D Gaussian Splatting for Inverse Rendering
                  </papertitle>
                </a>
                <br>
                <a href="https://lzhnb.github.io/">Zhihao Liang*</a>,
                <strong>Qi Zhang*</strong>,
                <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=zh-CN">Ying Shan</a>,
                <a href="http://kuijia.site/">Kui Jia</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                <a href="https://lzhnb.github.io/project-pages/gs-ir.html"><i class="fa fa-home"></i>Project Page</a>
                /
                <a href="https://arxiv.org/abs/2311.16473"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                /
                <a href="https://github.com/lzhnb/GS-IR"><i class="fa fa-github"></i>Code</a>
                <p></p>
                <p>
                  We propose GS-IR, a novel inverse rendering approach based on 3D Gaussian Splatting (GS) 
                  that leverages forward mapping volume rendering to achieve photorealistic novel view 
                  synthesis and relighting results.
                </p>
              </td>
            </tr>

              <!-- HumanNorm -->
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2024</div>
                    <img src="images/HumanNorm.gif" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://xhuangcv.github.io/">Xin Huang*</a>,
                  <a href="https://dsaurus.github.io/saurus/">Ruizhi Shao*</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://zhanghongwen.cn/">Hongwen Zhang</a>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://www.liuyebin.com/">Yebin Liu</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://humannorm.github.io/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2310.01406"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://humannorm.github.io/"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We propose HumanNorm, a novel approach for high-quality and realistic 3D human generation by learning the normal diffusion model 
                    including a normal-adapted diffusion model and a normal-aligned diffusion model. 
                    <!-- The normal-adapted diffusion model can generate high-fidelity normal maps corresponding to prompts 
                    with view-dependent text. The normal-aligned diffusion model learns to generate color images aligned 
                    with the normal maps, thereby transforming physical geometry details into realistic appearance. -->
                  </p>
                </td>
              </tr>

              <!-- FINER -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2024</div>
                    <img src="images/finer.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://lukeju.github.io/pyramid-website/">
                    <papertitle>FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions
                    </papertitle>
                  </a>
                  <br>
                  <a href="">Zhen Liu*</a>,
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu*</a>,
                  <strong>Qi Zhang</strong>,
                  <a> Jingde Fu</a>,
                  <a>Weibing Deng</a>
                  <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a>,
                  <a>Yanwen Guo</a>
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2312.02434"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2312.02434"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="https://arxiv.org/abs/2312.02434"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We have identified that this frequency-related problem can be greatly alleviated by introducing variable-periodic activation functions, for which we propose FINER.
                  </p>
                </td>
              </tr>
              
              <!-- HumanRef -->
              <td style="padding:20px;width:25%;vertical-align:middle;">
                <div class="paper-box">
                  <div class="badge">CVPR 2024</div>
                  <img src="images/humanref.png" width="300">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <papertitle>HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion
                  </papertitle>
                </a>
                <br>
                <a href="">Jingbo Zhang</a>,
                <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                <strong>Qi Zhang</strong>,
                <a href="">Yanpei Cao</a>
                <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
                <a href="https://liaojing.github.io/html/index.html">Jing Liao</a>
                <br>
                <em>CVPR</em>, 2024
                <br>
                <a href="https://eckertzhang.github.io/HumanRef.github.io/"><i class="fa fa-home"></i>Project Page</a>
                /
                <a href="https://arxiv.org/abs/2311.16961"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                /
                <a href="https://github.com/eckertzhang/HumanRef"><i class="fa fa-github"></i>Code</a>
                <p></p>
                <p>
                  HumanRef, a reference-guided 3D human generation framework, is capable of generating 3D clothed human with realistic, 
                  view-consistent texture and geometry from a single image input with the help of stable diffusion model.
                </p>
              </td>
            </tr>

            <!-- ConTex-Human -->
            <td style="padding:20px;width:25%;vertical-align:middle;">
              <div class="paper-box">
                <div class="badge">CVPR 2024</div>
                <img src="images/ConTex-Human.png" width="300">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis
                </papertitle>
              </a>
              <br>
              <a href="">Xiangjun Gao</a>,
              <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
              <a href="">Chaopeng Zhang</a>,
              <strong>Qi Zhang</strong>,
              <a href="">Yanpei Cao</a>,
              <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=ZMLhZJ8AAAAJ&view_op=list_works">Long Quan</a>
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://gaoxiangjun.github.io/contex_human/"><i class="fa fa-home"></i>Project Page</a>
              /
              <a href="https://arxiv.org/abs/2311.17123"><i class="fa fa-file-pdf-o"></i>arXiv</a>
              /
              <a href="https://github.com/gaoxiangjun/ConTex-Human"><i class="fa fa-github"></i>Code</a>
              <p></p>
              <p>
                In this paper, we introduce a texture-consistent back view synthesis module that could transfer the reference image 
                content to the back view through depth and text-guided attention injection with the help of stable diffusion model.
              </p>
            </td>
          </tr>

              <!-- NeIF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">AAAI 2024</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/NeAI.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yiyuzhuang.github.io/NeAI/">
                    <papertitle>NeIF: A Pre-convolved Representation for Plug-and-Play Neural Illumination Fields
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://yiyuzhuang.github.io/NeAI/">Yiyu Zhuang*</a>,
                  <strong>Qi Zhang*</strong>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                  <a href="https://zhuhao-nju.github.io/">Hao Zhu</a>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
                  <br>
                  <em>AAAI</em>, 2024
                  <br>
                  <a href="https://yiyuzhuang.github.io/NeAI/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2304.08757"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://yiyuzhuang.github.io/NeAI/"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We propose a fully differentiable framework named neural ambient illumination (NeAI) that uses
                    Neural Radiance Fields (NeRF) as a lighting model to handle complex lighting in a physically based
                    way.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">TPAMI 2024</div>
                    <img src="images/diner.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ezio77.github.io/DINER-website/">
                    <papertitle>Disorder-invariant Implicit Neural Representation</papertitle>
                  </a>
                  <br>
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu*</a>,
                  <a href="https://ezio77.github.io/DINER-website/">Shaowen Xie*</a>,
                  <a href="https://ezio77.github.io/DINER-website/">Zhen Liu*</a>,
                  <a >Fengyi Liu</a>
                  <strong>Qi Zhang</strong>,
                  <a href="https://zhouyou-nju.github.io/">You Zhou</a>,
                  <a href="https://ezio77.github.io/DINER-website/">Yi Lin</a>,
                  <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2024
                  <br>
                  <a href="https://ezio77.github.io/DINER-website/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2211.07871"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/Ezio77/DINER"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    In this paper, we find that such a frequency-related problem could be largely solved by re-arranging
                    the coordinates of the input signal, for which we propose the disorder-invariant implicit neural
                    representation (DINER) by augmenting a hash-table to a traditional INR backbone.
                  </p>
                </td>
              </tr>
              

              <!-- LoD-NeuS -->
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">SIGGRAPH Asia 2023</div>
                    <img src="images/lod-neus.jpg" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="">
                    <papertitle>Anti-Aliased Neural Implicit Surfaces with Encoding Level of Detail
                    </papertitle>
                  </a>
                  <br>
                  <a href="">Yiyu Zhuang*</a>,
                  <strong>Qi Zhang*</strong>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://zhuhao-nju.github.io/">Hao Zhu</a>,
                  <a href="">Yao Yao</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="">Yanpei Cao</a>,
                  <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
                  <br>
                  <em>SIGGRAPH Asia</em>, 2023
                  <br>
                  <a href="https://nju-3dv.github.io/projects/lodneus/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2309.10336"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/yiyuzhuang/LoD-NeuS"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    Our method, called LoD-NeuS, adaptively encodes Level of Detail (LoD) features derived from
                    the multi-scale and multi-convoluted tri-plane representation. By optimizing a neural Signal Distance
                    Field (SDF), our method is capable of reconstructing high-fidelity geometry
                  </p>
                </td>
              </tr>

              <!-- Pramid-NeRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">IJCV 2023</div>
                    <img src="images/ijcv_pyramid_NeRF.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://lukeju.github.io/pyramid-website/">
                    <papertitle>Pyramid NeRF: Frequency Guided Fast Radiance Field Optimization
                    </papertitle>
                  </a>
                  <br>
                  <a href="">Junyu Zhu*</a>,
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu*</a>,
                  <strong>Qi Zhang</strong>,
                  <a>Fang Zhu</a>
                  <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
                  <br>
                  <em>International Journal of Computer Vision (IJCV)</em>, 2023
                  <br>
                  <a href="https://lukeju.github.io/pyramid-website/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://link.springer.com/article/10.1007/s11263-023-01829-3"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="https://github.com/lukeju/Pyramid-NeRF"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    In this paper, we propose the Pyramid NeRF, which guides the NeRF training in a 'low-frequency
                    first, high-frequency second' style using the image pyramids and could improve the training and
                    inference speed at 15x and 805x, respectively.
                  </p>
                </td>
              </tr>

              <!-- NeAI -->
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">arXiv 2023</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/NeAI.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://yiyuzhuang.github.io/NeAI/">
                    <papertitle>NeAI: A Pre-convoluted Representation for Plug-and-Play Neural Ambient Illumination
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://yiyuzhuang.github.io/NeAI/">Yiyu Zhuang*</a>,
                  <strong>Qi Zhang*</strong>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                  <a href="https://zhuhao-nju.github.io/">Hao Zhu</a>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
                  <br>
                  <em>arXiv</em>, 2023
                  <br>
                  <a href="https://yiyuzhuang.github.io/NeAI/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2304.08757"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://yiyuzhuang.github.io/NeAI/"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We propose a fully differentiable framework named neural ambient illumination (NeAI) that uses
                    Neural Radiance Fields (NeRF) as a lighting model to handle complex lighting in a physically based
                    way.
                  </p>
                </td>
              </tr> -->

              <!-- WFOV -->
              <tr onmouseout="wfov_stop()" onmouseover="wfov_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='wfov_image'>
                      <div class="badge">CVPR 2023</div>
                      <img src="images/GOPR0065_optImgQCPolar.jpg" width="300">
                    </div>
                    <div class="badge">CVPR 2023</div>
                    <img src='images/GOPR0065.JPG' width="300">
                    <!-- <div class="two" id='wfov_image'>
                      <div class="badge">CVPR 2023</div>
                      <img src="images/GOPR0077_optImgQCPolar.jpg" width="300">
                    </div>
                    <div class="badge">CVPR 2023</div>
                    <img src='images/GOPR0077.JPG' width="300"> -->
                  </div>
                  <script type="text/javascript">
                    function wfov_start() {
                      document.getElementById('wfov_image').style.opacity = "1";
                    }

                    function wfov_stop() {
                      document.getElementById('wfov_image').style.opacity = "0";
                    }
                    wfov_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://qzhang-cv.github.io/">
                    <papertitle>Wide-angle Rectification via Content-aware Conformal Mapping</papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href=""><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Wide-Angle_Rectification_via_Content-Aware_Conformal_Mapping_CVPR_2023_paper.pdf"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  <p></p>
                  <p>
                    We propose a new content-aware optimization framework to preserve both local conformal shape (e.g.
                    face or salient regions) and global linear structures (straight lines).
                  </p>
                </td>
              </tr>

              <!-- Neural Camera -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/NeuralCamera.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://xhuangcv.github.io/neucam/">
                    <papertitle>Inverting the Imaging Process by Learning an Implicit Camera Model</papertitle>
                  </a>
                  <br>
                  <a href="https://xhuangcv.github.io/">Xin Huang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://xhuangcv.github.io/neucam/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2304.12748"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/xhuangcv/neucam/"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    this paper proposes a novel implicit camera model which represents the physical imaging process of a
                    camera as a deep neural network. We demonstrate the power of this new implicit camera model on two
                    inverse imaging tasks: i) generating all-in-focus photos, and ii) HDR imaging.
                  </p>
                </td>
              </tr>

              <!-- LIRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/LIRF.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://xhuangcv.github.io/lirf/">
                    <papertitle>Local Implicit Ray Function for Generalizable Radiance Field Representation</papertitle>
                  </a>
                  <br>
                  <a href="https://xhuangcv.github.io/">Xin Huang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://xhuangcv.github.io/lirf/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2304.12746"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/xhuangcv/lirf/"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We propose LIRF (Local Implicit Ray Function), a generalizable neural rendering approach for novel
                    view rendering. Given 3D positions within conical frustums, LIRF takes 3D coordinates and the
                    features of conical frustums as inputs and predicts a local volumetric radiance field.
                  </p>
                </td>
              </tr>

              <!-- Diner -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023 (Highlight)</div>
                    <img src="images/diner.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ezio77.github.io/DINER-website/">
                    <papertitle>DINER: Disorder-Invariant Implicit Neural Representation</papertitle>
                  </a>
                  <br>
                  <a href="https://ezio77.github.io/DINER-website/">Shaowen Xie*</a>,
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu*</a>,
                  <a href="https://ezio77.github.io/DINER-website/">Zhen Liu*</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://zhouyou-nju.github.io/">You Zhou</a>,
                  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
                  <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://ezio77.github.io/DINER-website/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2211.07871"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/Ezio77/DINER"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    In this paper, we find that such a frequency-related problem could be largely solved by re-arranging
                    the coordinates of the input signal, for which we propose the disorder-invariant implicit neural
                    representation (DINER) by augmenting a hash-table to a traditional INR backbone.
                  </p>
                </td>
              </tr>

              <!-- L2G-NeRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/l2g-nerf.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://rover-xingyu.github.io/L2G-NeRF/">
                    <papertitle>Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  <a href="https://fanegg.github.io/">Yue Chen</a>
                  <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://yuguo-xjtu.github.io/">Yu Guo</a>,
                  <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en">Ying Shan</a>,
                  <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Fei Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://rover-xingyu.github.io/L2G-NeRF/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2211.11505"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/rover-xingyu/L2G-NeRF"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We propose L2G-NeRF, a Local-to-Global registration method for bundle-adjusting Neural Radiance
                    Fields, including the pixel-wise local alignment and the frame-wise global alignment.
                  </p>
                </td>
              </tr>

              <!-- UV Volume -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/uv-Volume.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://fanegg.github.io/UV-Volumes/">
                    <papertitle>UV Volumes for Real-time Rendering of Editable Free-view Human Performance</papertitle>
                  </a>
                  <br>
                  <a href="https://fanegg.github.io/">Yue Chen</a>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang*</a>,
                  <a href="https://rover-xingyu.github.io/">Xingyu Chen</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://yuguo-xjtu.github.io/">Yu Guo</a>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>,
                  <a href="http://www.aiar.xjtu.edu.cn/info/1046/1242.htm">Fei Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://fanegg.github.io/UV-Volumes/"><i class="fa fa-home"></i>Project Page</a> /
                  <a href="https://arxiv.org/abs/2203.14402"><i class="fa fa-file-pdf-o"></i>arXiv</a> /
                  <a href="https://github.com/fanegg/UV-Volumes"><i class="fa fa-github"></i>Code</a> /
                  <a href="https://youtu.be/JftQnXLMmPc">Video</a>
                  <p></p>
                  <p>
                    We propose the UV Volumes, a new approach that can achieve real-time rendering, and editable NeRF,
                    decomposing a dynamic human into 3D UV Volumes and a 2D appearance texture.
                  </p>
                </td>
              </tr>

              <!-- fice swapping -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2023</div>
                    <img src="images/e4s_teaser.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://e4s2022.github.io/">
                    <papertitle>Fine-Grained Face Swapping via Regional GAN Inversion</papertitle>
                  </a>
                  <br>
                  <a href="https://e4s2022.github.io/">Zhian Liu*</a>,
                  <a href="https://e4s2022.github.io/">Maomao Li*</a>,
                  <a href="https://yzhang2016.github.io/">Yong Zhang*</a>,
                  <a href="https://e4s2022.github.io/">Cairong Wang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>,
                  <a href="https://nieyongwei.net/">Yongwei Nie</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
                  <br>
                  <a href="https://e4s2022.github.io/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2211.14068"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/e4s2022/e4s"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We present a novel paradigm for high-fidelity face swapping that faithfully preserves the desired
                    subtle geometry and texture details.
                  </p>
                </td>
              </tr>

              <!-- Neural Face Editing -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">SIGGRAPH 2022</div>
                    <img src="images/nep_teaser.jpg" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://limacv.github.io/neuvf_web/">
                    <papertitle>Neural Parameterization for Dynamic Human Head Editing</papertitle>
                  </a>
                  <br>
                  <a href="https://limacv.github.io/homepage/">Li Ma</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://liaojing.github.io/html/index.html">Jing Liao</a>,
                  <a href="https://xuanwangvc.github.io/">Xuan Wang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>,
                  <a href="https://home.cse.ust.hk/~psander/">Pedro V. Sander</a>
                  <br>
                  <em>ACM Transactions on Graphics</em>, 2022
                  <br>
                  <a href="https://limacv.github.io/neuvf_web/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2111.14292"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/limacv/NeUVF"><i class="fa fa-github"></i>Code</a>
                  <p></p>
                  <p>
                    We try to introduce explicit parameters into implicit dynamic NeRF representations to achieve
                    editing of 3D human heads.
                  </p>
                </td>
              </tr>

              <tr onmouseout="mhi_stop()" onmouseover="mhi_start()">
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="one">
                    <div class="badge">arXiv 2022</div>
                    <img src='images/mhi.png' width="300">
                  </div>
                  <script type="text/javascript">
                    function mhi_start() {
                      document.getElementById('mhi_image').style.opacity = "1";
                    }

                    function mhi_stop() {
                      document.getElementById('mhi_image').style.opacity = "0";
                    }
                    mhi_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2204.00156">
                    <papertitle>Stereo Unstructured Magnification: Multiple Homography Image for View Synthesis
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang*</strong>,
                  <a href="https://arxiv.org/abs/2204.00156">Xin Huang*</a>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,,
                  <a href="https://teacher.nwpu.edu.cn/xwang.html">Xue Wang</a>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>arXiv</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2204.00156"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  <p></p>
                  <p>We propose a novel multiple homography image (MHI) representation, comprising of a set of scene
                    planes with fixed normals and distances, for view synthesis from stereo images.
                    <!-- The key to our method is to model the physical imaging process, which dictates that the radiance of a 
                      scene point transforms to a pixel value in the LDR image with two implicit functions: a radiance field and a tone mapper.</p> -->
                </td>
              </tr>

              <!-- HDR-NeRF -->
              <tr onmouseout="hdr_nerf_stop()" onmouseover="hdr_nerf_start()">
                <td style="padding:20px;width:25%;vertical-align:middle;">
                  <div class="paper-box">
                    <div class="badge">CVPR 2022</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/HDR-NeRF-Wide.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://xhuangcv.github.io/hdr-nerf/">
                    <papertitle>HDR-NeRF: High Dynamic Range Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  <a href="https://xhuangcv.github.io/">Xin Huang</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="">Xuan Wang</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://xhuangcv.github.io/hdr-nerf/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2111.14451"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="https://github.com/xhuangcv/hdr-nerf"><i class="fa fa-github"></i>Code</a>
                  /
                  <a
                    href="https://drive.google.com/drive/folders/1OTDLLH8ydKX1DcaNpbQ46LlP0dKx6E-I?usp=sharing">Dataset</a>
                  /
                  <a href="https://www.youtube.com/watch?v=GmxsW9L1O6s&ab_channel=XinHuang">video</a>
                  <p></p>
                  <p>We present High Dynamic Range Neural Radiance Fields (HDR-NeRF) to recover an HDR radiance field
                    from a set of low dynamic range (LDR) views with different exposures.
                </td>
              </tr>

              <!-- Ha-NeRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="paper-box">
                    <div class="badge">CVPR 2022</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/ha-nerf.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://rover-xingyu.github.io/Ha-NeRF/">
                    <papertitle>Hallucinated Neural Radiance Fields in the Wild</papertitle>
                  </a>
                  <br>
                  <a href="https://rover-xingyu.github.io/Ha-NeRF/">Xingyu Chen</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://rover-xingyu.github.io/Ha-NeRF/">Yue Chen</a>,
                  <a href="https://scholar.google.com/citations?user=PhkrqioAAAAJ&hl=en">Ying Feng</a>,
                  <a href="https://rover-xingyu.github.io/Ha-NeRF/">Xuan Wang</a>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://rover-xingyu.github.io/Ha-NeRF/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2111.15246"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  <p></p>
                  <p>
                    This paper studies the problem of hallucinated NeRF: i.e. recovering a realistic NeRF at a different
                    time of day from a group of tourism images.</p>
                </td>
              </tr>

              <!-- Deblur-NeRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="paper-box">
                    <div class="badge">CVPR 2022</div>
                    <video width="300" muted autoplay loop>
                      <source src="images/deblur-nerf.mp4" type="video/mp4">
                    </video>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2111.14292">
                    <papertitle>Deblur-NeRF: Neural Radiance Fields from Blurry Images</papertitle>
                  </a>
                  <br>
                  <a href="https://arxiv.org/abs/2111.14292">Li Ma</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <a href="https://liaojing.github.io/html/index.html">Jing Liao</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://arxiv.org/abs/2111.14292">Xuan Wang</a>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>,
                  <a href="https://home.cse.ust.hk/~psander/">Pedro V. Sander</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://limacv.github.io/deblurnerf/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2111.14292"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  <p></p>
                  <p>
                    In this paper, we propose Deblur-NeRF, the first method that can recover a sharp NeRF from blurry
                    input. A novel Deformable Sparse Kernel (DSK) module is presented for both camera motion blur and
                    defocus blur.</p>
                </td>
              </tr>

              <!-- FENeRF -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="paper-box">
                    <div class="badge">CVPR 2022</div>
                    <img src="images/fenerf.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2111.15490">
                    <papertitle>FENeRF: Face Editing in Neural Radiance Fields</papertitle>
                  </a>
                  <br>
                  <a href="https://arxiv.org/abs/2111.15490">Jingxiang Sun</a>,
                  <a href="https://arxiv.org/abs/2111.15490">Xuan Wang</a>,
                  <a href="https://yzhang2016.github.io/yongnorriszhang.github.io/">Yong Zhang</a>,
                  <a href="https://xiaoyu258.github.io/">Xiaoyu Li</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://liuyebin.com/">Yebin Liu</a>,
                  <a href="https://juewang725.github.io/">Jue Wang</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
                  <br>
                  <a href="https://mrtornado24.github.io/FENeRF/"><i class="fa fa-home"></i>Project Page</a>
                  /
                  <a href="https://arxiv.org/abs/2111.15490"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  <p></p>
                  <p>
                    A 3D-aware generator (FENeRF) is prorposed to produce view-consistent and locally-editable portrait
                    images.
                    <!-- Two decoupled latent codes are used to generate corresponding facial semantics and texture. -->
                  </p>
                </td>
              </tr>

              <!-- RSEG -->
              <tr bgcolor="#ffffd0">
                <td style="padding:10px;width:25%;vertical-align:top">
                  <div class="paper-box">
                    <div class="badge">TPAMI 2022</div>
                    <img src="images/TPAMI2020_tarser.png" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9204467">
                    <papertitle>Ray-Space Epipolar Geometry for Light Field Cameras</papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2022
                  <br>
                  <a href="data/TPAMI2020RSEG.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/zhang2020RSEG.bib">bibtex</a>
                  <p></p>
                  <p>This paper fills in this gap by developing a novel ray-space epipolar geometry which intrinsically
                    encapsulates the complete projective relationship between two light fields.
                    Ray-space fundamental matrix and its properties are then derived to constrain ray-ray
                    correspondences for general and special motions.
                  </p>
                </td>
              </tr>

              <!-- Self-Calibration of LFC -->
              <tr onmouseout="lfc_3d_stop()" onmouseover="lfc_3d_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='lfc_3d_image'>
                      <div class="badge">IJCV 2021</div>
                      <img src="images/lfc_3d_pipeline.jpg" width="300">
                    </div>
                    <div class="badge">IJCV 2021</div>
                    <img src='images/lfc_3d_tarser.jpg' width="300">
                  </div>
                  <script type="text/javascript">
                    function lfc_3d_start() {
                      document.getElementById('lfc_3d_image').style.opacity = "1";
                    }

                    function lfc_3d_stop() {
                      document.getElementById('lfc_3d_image').style.opacity = "0";
                    }
                    lfc_3d_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/article/10.1007/s11263-021-01516-1">
                    <papertitle>3D Scene Reconstruction with an Un-calibrated Light Field Camera</papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  <a href="https://teacher.nwpu.edu.cn/xwang.html">Xue Wang</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>International Journal of Computer Vision (IJCV)</em>, 2021
                  <br>
                  <a href="data/IJCV2021.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/zhang20213d.bib">bibtex</a>
                  <p></p>
                  <p>This paper is concerned with the problem of multi-view 3D reconstruction with an un-calibrated
                    micro-lens array based light field camera..</p>
                </td>
              </tr>

              <!-- lf_salient -->
              <tr onmouseout="lf_salient_stop()" onmouseover="lf_salient_start()">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="paper-box">
                    <div class="badge">MTA 2021</div>
                    <img src="images/lf_salient.webp" width="300">
                    <img src="images/lf_salient2.webp" width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/article/10.1007/s11042-020-08890-x">
                    <papertitle>Region-based Depth Feature Descriptor for Saliency Detection on Light Field</papertitle>
                  </a>
                  <br>
                  <a href="https://teacher.nwpu.edu.cn/xwang.html">Xue Wang</a>,
                  <a href="https://link.springer.com/article/10.1007/s11042-020-08890-x">Yingying Dong</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>Multimedia Tools and Applications</em>, 2021
                  <br>
                  <a href="data/MTA2021.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/wang2021region.bib">bibtex</a>
                  <p></p>
                  <p>
                    By reinterpreting the usage of dark channels in estimating the amount of defocus, a novel
                    region-based depth feature descriptor (RDFD) defined over the focal stack is proposed.</p>
                </td>
              </tr>

              <!-- light field optical flow -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="small-box">
                    <div class="badge">TCI 2029</div>
                    <video width="200" muted autoplay loop>
                      <source src="images/tci_2019.mp4" type="video/mp4">
                    </video>
                    <!-- <img src='images/tci_2019_taster.jpg' width="300"> -->
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8637789">
                    <papertitle>Full View Optical Flow Estimation Leveraged From Light Field Superpixel</papertitle>
                  </a>
                  <br>
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu</a>,
                  <a href="https://ieeexplore.ieee.org/abstract/document/8637789">Xiaoming Sun</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>,
                  Antonio Robles-Kelly,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>,
                  Shaodi You
                  <br>
                  <em>IEEE Transactions on Computational Imaging</em>, 2019
                  <br>
                  <a href="data/TCI2019.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/zhu2019full.bib">bibtex</a>
                  <p></p>
                  <p>
                    Our method employs the structure delivered by the four-dimensional light field over multiple views
                    making use of superpixels for a full view optical flow estiamtion.</p>
                </td>
              </tr>

              <!-- RSP -->
              <tr onmouseout="rsp_stop()" onmouseover="rsp_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='rsp_image'>
                      <div class="badge">CVPR 2019</div>
                      <img src="images/cvpr2019_res.jpg" width="300">
                    </div>
                    <div class="badge">CVPR 2019</div>
                    <img src='images/RSP.png' width="300">
                  </div>
                  <script type="text/javascript">
                    function rsp_start() {
                      document.getElementById('rsp_image').style.opacity = "1";
                    }

                    function rsp_stop() {
                      document.getElementById('rsp_image').style.opacity = "0";
                    }
                    rsp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9204467">
                    <papertitle>Ray-Space Projection Model for Light Field Camera</papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a
                    href="https://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Ray-Space_Projection_Model_for_Light_Field_Camera_CVPR_2019_paper.html">Jinbo
                    Ling</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>,
                  <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
                  <br>
                  <a href="data/CVPR2019.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/CVPR2019_supp.pdf">Supp</a>
                  /
                  <a href="data/zhang2019RSP.bib">bibtex</a>
                  <p></p>
                  <p>In the paper, we propose a novel ray-space projection model to transform sets of rays captured by
                    multiple light field cameras in term of the Plucker coordinates.</p>
                </td>
              </tr>

              <!-- TIP 2019 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="badge">TIP 2019</div>
                    <img src='images/TIP2019_tarser.jpg' width="300">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/8763926">
                    <papertitle>4D Light Field Superpixel and Segmentation</papertitle>
                  </a>
                  <br>
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu</a>,
                  <strong>Qi Zhang</strong>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>,
                  <a href="http://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>
                  <br>
                  <em>IEEE Transactions on Image Processing (TIP)</em>, 2019
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2017
                  <br>
                  <a href="data/TIP2019.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/zhu20194d.bib">bibtex</a>
                  <p></p>
                  <p>The light field superpixel (LFSP) is first defined mathematically and then a refocus-invariant
                    metric named LFSP self-similarity is proposed to evaluate the segmentation performance.</p>
                </td>
              </tr>

              <!-- ACCV -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="badge">ACCV 2018</div>
                  <img src='images/accv2018_tarser.png' width="200">
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-030-20876-9_2">
                    <papertitle>Common Self-polar Triangle of Concentric Conics for Light Field Camera Calibration
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>Asian Conference on Computer Vision</em>, 2018
                  <br>
                  <a href="data/ACCV2018.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="">Supp</a>
                  /
                  <a href="data/zhang2018Common.bib">bibtex</a>
                  <p></p>
                  <p>Instead of a planar checkerboard, we propose to calibrate light field camera using a concentric
                    conics pattern based on the property and reconstruction of common self-polar triangle with respect
                    to concentric circle and ellipse..</p>
                </td>
              </tr>

              <!-- MPC -->
              <tr onmouseout="mpc_stop()" onmouseover="mpc_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mpc_image'>
                      <div class="badge">TPAMI 2019</div>
                      <img src="images/TPAMI2018_LFC.jpg" width="300">
                    </div>
                    <div class="badge">TPAMI 2019</div>
                    <img src='images/TPAMI2018_Tarser.png' width="300">
                  </div>
                  <script type="text/javascript">
                    function mpc_start() {
                      document.getElementById('mpc_image').style.opacity = "1";
                    }

                    function mpc_stop() {
                      document.getElementById('mpc_image').style.opacity = "0";
                    }
                    mpc_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8430574">
                    <papertitle>A Generic Multi-Projection-Center Model and Calibration Method for Light Field Camera
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qi Zhang</strong>,
                  <a href="https://ieeexplore.ieee.org/abstract/document/8430574">Chunping Zhang</a>,
                  <a href="https://ieeexplore.ieee.org/abstract/document/8430574">Jinbo Ling</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>,
                  <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
                  <br>
                  <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2019
                  <br>
                  <a href="https://arxiv.org/abs/1808.02244"><i class="fa fa-file-pdf-o"></i>arXiv</a>
                  /
                  <a href="data/TPAMI2018.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="https://codeocean.com/capsule/9914149/tree/v2"><i class="fa fa-github"></i>Code</a>
                  /
                  <a href="data/zhang2018MPC.bib">bibtex</a>
                  <p></p>
                  <p>The MPC model can generally parameterize light field in different imaging formations, including
                    conventional and focused light field cameras.</p>
                </td>
              </tr>

              <!-- ICIP -->
              <!-- <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="badge">ICIP 2017</div>
                  <img src='images/ICIP2017_tarser.gif' width=300>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8296463">
                    <papertitle>Extending the FOV from disparity and color consistencies in multiview light fields
                    </papertitle>
                  </a>
                  <br>
                  <a href="http://zhaoren.one/">Zhao Ren</a>
                  <strong>Qi Zhang</strong>,
                  <a href="https://pakfa.github.io/zhuhao_photo.github.io/">Hao Zhu</a>,
                  <a href="https://teacher.nwpu.edu.cn/qwang.html">Qing Wang</a>
                  <br>
                  <em>IEEE International Conference on Image Processing (ICIP)</em>, 2017
                  <br>
                  <a href="data/ICIP2017.pdf"><i class="fa fa-file-pdf-o"></i>PDF</a>
                  /
                  <a href="data/ren2017extending.bib">bibtex</a>
                  <p></p>
                  <p>In contrast to previous methods, our algorithm is the first that achieves light field registration
                    and rendering based on epipolar plane image (EPI) properties, including disparity and color
                    consistencies. </p>
                </td>
              </tr> -->

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <heading>Other Publications</heading>
                      <p>
                        <strong>Qi Zhang</strong>, Qing Wang. Common self-polar triangle of separate circles for light
                        field
                        camera calibration[J]. Xibei Gongye Daxue Xuebao/Journal of Northwestern Polytechnical
                        University,
                        2021. <br>
                        Yaning Li, <strong>Qi Zhang</strong>, Xue Wang, Qing Wang. Light Field SLAM based on Ray-Space
                        Projection Model[C]. Optoelectronic Imaging and Multimedia Technology VI, 2019. <br>
                        <strong>Qi Zhang</strong>, Xue Wang, Qing Wang. Light Field Planar Homography and Its
                        Application[C]. Optoelectronic Imaging and Multimedia Technology VI, 2019. <br>
                        Zhao Ren, <strong>Qi Zhang</strong>, Hao Zhu, Qing Wang. Extending the FOV from disparity and
                        color consistencies in multiview light fields[C]. IEEE International Conference on Image
                        Processing (ICIP), 2017 <br>
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <tr>
                    <td style="padding:0px">
                      <br>
                      <p style="text-align:right;font-size:small;">
                        This template is a modification to Jon Barron's <a
                          href="https://github.com/jonbarron/jonbarron_website">website</a>.
                        Feel free to clone it for your own use while attributing the original author <a
                          href="https://jonbarron.info/">Jon Barron</a>.
                      </p>
                    </td>
                  </tr>
                </tbody>
              </table>
        </td>
      </tr>
  </table>
</body>

</html>